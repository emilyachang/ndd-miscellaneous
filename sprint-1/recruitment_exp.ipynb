{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recruitment Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developing lifelong learning algorithms, prior work has involved two main approaches: building and reallocating. Building involves adding new resources to support the arrival of new data, whereas reallocation involves compression of representations to make room for new ones. However, biologically, there is a spectrum between these two modes.\n",
    "\n",
    "In order to examine whether current resources could be better leveraged, we test a range of approaches: **recruitment** of the best-performing existing trees, **building** new trees completely (the default approach that our L2F uses), ignoring all prior trees (essentially an uncertainty forest), and a **hybrid** between building and recruitment.\n",
    "\n",
    "We examine the performance of these four approaches based on the available training sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from math import log2, ceil \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(action='once')\n",
    "\n",
    "\n",
    "from proglearn.progressive_learner import ClassificationProgressiveLearner\n",
    "from proglearn.forest import LifelongClassificationForest, UncertaintyForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR 10x10 Tasks\n",
    "\n",
    "The classification problem that we examine in this tutorial makes use of the CIFAR 10x10 dataset. This dataset contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "### Main hyperparameters ###\n",
    "############################\n",
    "ntrees = 50\n",
    "hybrid_comp_trees = 25\n",
    "estimation_set = 0.63\n",
    "validation_set= 1-estimation_set\n",
    "\n",
    "#num_points_per_task = 5000\n",
    "#num_points_per_forest = 500\n",
    "#reps = 30\n",
    "num_points_per_task = 100\n",
    "num_points_per_forest = 10\n",
    "reps = 5\n",
    "\n",
    "task_10_sample = 10*np.array([10, 50, 100, 200, 350, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(data_x, data_y, num_points_per_task, total_task=10, shift=1):\n",
    "    x = data_x.copy()\n",
    "    y = data_y.copy()\n",
    "    idx = [np.where(data_y == u)[0] for u in np.unique(data_y)]\n",
    "    train_x_across_task = []\n",
    "    train_y_across_task = []\n",
    "    test_x_across_task = []\n",
    "    test_y_across_task = []\n",
    "\n",
    "    batch_per_task=5000//num_points_per_task\n",
    "    sample_per_class = num_points_per_task//total_task\n",
    "    test_data_slot=100//batch_per_task\n",
    "\n",
    "    for task in range(total_task):\n",
    "        for batch in range(batch_per_task):\n",
    "            for class_no in range(task*10,(task+1)*10,1):\n",
    "                indx = np.roll(idx[class_no],(shift-1)*100)\n",
    "                \n",
    "                if batch==0 and class_no==task*10:\n",
    "                    train_x = x[indx[batch*sample_per_class:(batch+1)*sample_per_class],:]\n",
    "                    train_y = y[indx[batch*sample_per_class:(batch+1)*sample_per_class]]\n",
    "                    test_x = x[indx[batch*test_data_slot+500:(batch+1)*test_data_slot+500],:]\n",
    "                    test_y = y[indx[batch*test_data_slot+500:(batch+1)*test_data_slot+500]]\n",
    "                else:\n",
    "                    train_x = np.concatenate((train_x, x[indx[batch*sample_per_class:(batch+1)*sample_per_class],:]), axis=0)\n",
    "                    train_y = np.concatenate((train_y, y[indx[batch*sample_per_class:(batch+1)*sample_per_class]]), axis=0)\n",
    "                    test_x = np.concatenate((test_x, x[indx[batch*test_data_slot+500:(batch+1)*test_data_slot+500],:]), axis=0)\n",
    "                    test_y = np.concatenate((test_y, y[indx[batch*test_data_slot+500:(batch+1)*test_data_slot+500]]), axis=0)\n",
    "        \n",
    "        train_x_across_task.append(train_x)\n",
    "        train_y_across_task.append(train_y)\n",
    "        test_x_across_task.append(test_x)\n",
    "        test_y_across_task.append(test_y)\n",
    "\n",
    "    return train_x_across_task, train_y_across_task, test_x_across_task, test_y_across_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data \n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "data_x = np.concatenate([X_train, X_test])\n",
    "data_x = data_x.reshape((data_x.shape[0], data_x.shape[1] * data_x.shape[2] * data_x.shape[3]))\n",
    "data_y = np.concatenate([y_train, y_test])\n",
    "data_y = data_y[:, 0]\n",
    "\n",
    "train_x_across_task, train_y_across_task, test_x_across_task, test_y_across_task = sort_data(\n",
    "    data_x,data_y,num_points_per_task\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voter_predict_proba(voter, nodes_across_trees):\n",
    "            def worker(tree_idx):\n",
    "                #get the node_ids_to_posterior_map for this tree\n",
    "                node_ids_to_posterior_map = voter.tree_idx_to_node_ids_to_posterior_map[tree_idx]\n",
    "\n",
    "                #get the nodes of X\n",
    "                nodes = nodes_across_trees[tree_idx]\n",
    "\n",
    "                posteriors = []\n",
    "                node_ids = node_ids_to_posterior_map.keys()\n",
    "\n",
    "                #loop over nodes of X\n",
    "                for node in nodes:\n",
    "                    #if we've seen this node before, simply get the posterior\n",
    "                    if node in node_ids:\n",
    "                        posteriors.append(node_ids_to_posterior_map[node])\n",
    "                    #if we haven't seen this node before, simply use the uniform posterior \n",
    "                    else:\n",
    "                        posteriors.append(np.ones((len(np.unique(voter.classes_)))) / len(voter.classes_))\n",
    "                return posteriors\n",
    "\n",
    "            if voter.parallel:\n",
    "                return Parallel(n_jobs=-1)(\n",
    "                                delayed(worker)(tree_idx) for tree_idx in range(voter.n_estimators)\n",
    "                        )\n",
    "\n",
    "            else:\n",
    "                return [worker(tree_idx) for tree_idx in range(voter.n_estimators)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_posteriors(l2f, X, representation = 0, decider = 0):\n",
    "        l2f.check_task_idx_(decider)\n",
    "        \n",
    "        if representation == \"all\":\n",
    "            representation = range(l2f.n_tasks)\n",
    "        elif isinstance(representation, int):\n",
    "            representation = np.array([representation])\n",
    "        \n",
    "        def worker(transformer_task_idx):\n",
    "            transformer = l2f.transformers_across_tasks[transformer_task_idx]\n",
    "            voter = l2f.voters_across_tasks_matrix[decider][transformer_task_idx]\n",
    "\n",
    "            return voter_predict_proba(voter,transformer(X))\n",
    "        \n",
    "        '''if l2f.parallel:\n",
    "            posteriors_across_tasks = np.array(\n",
    "                        Parallel(n_jobs=-1)(\n",
    "                                delayed(worker)(transformer_task_idx) for transformer_task_idx in representation\n",
    "                        )\n",
    "                )    \n",
    "        else:'''\n",
    "        posteriors_across_tasks = np.array([worker(transformer_task_idx) for transformer_task_idx in representation])    \n",
    "\n",
    "        return posteriors_across_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrices for storing values\n",
    "hybrid = np.zeros(reps,dtype=float)\n",
    "building = np.zeros(reps,dtype=float)\n",
    "recruiting= np.zeros(reps,dtype=float)\n",
    "uf = np.zeros(reps,dtype=float)\n",
    "mean_accuracy_dict = {'hybrid':[],'building':[],'recruiting':[],'UF':[]}\n",
    "std_accuracy_dict = {'hybrid':[],'building':[],'recruiting':[],'UF':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from paper:\n",
    "\n",
    "train L2F on first nine CIFAR 10x10 tasks (50 trees/task, 500 samples/task)\n",
    "\n",
    "for 10th task:\n",
    "1. recruiting = select 50/450 existing trees that perform best on task 10\n",
    "2. building = train 50 new trees (L2F default)\n",
    "3. hybrid = build and recruit 25 trees\n",
    "4. UF = ignore prior trees\n",
    "\n",
    "should see L2F outperform others except @ 5k training samples: \"relative performance depends on available resources and sample size\"\n",
    "\n",
    "future work: \"investigate optimal strtegies or determining how to optimally leverage existing resources given a new task\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from proglearn.progressive_learner import ClassificationProgressiveLearner\n",
    "from proglearn.transformers import TreeClassificationTransformer\n",
    "from proglearn.voters import TreeClassificationVoter\n",
    "from proglearn.deciders import SimpleArgmaxAverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine simpleargmaxaverage class to be\n",
    "class NOTAVERAGE(SimpleArgmaxAverage):\n",
    "    def predict_proba(self, X, transformer_ids=None):\n",
    "        vote_per_transformer_id = []\n",
    "        for transformer_id in (\n",
    "            transformer_ids\n",
    "            if transformer_ids is not None\n",
    "            else self.transformer_id_to_voters.keys()\n",
    "        ):\n",
    "            if not self.is_fitted():\n",
    "                msg = (\n",
    "                    \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n",
    "                    \"appropriate arguments before using this decider.\"\n",
    "                )\n",
    "                raise NotFittedError(msg % {\"name\": type(self).__name__})\n",
    "\n",
    "            vote_per_bag_id = []\n",
    "            for bag_id in range(len(self.transformer_id_to_transformers[transformer_id])):\n",
    "                transformer = self.transformer_id_to_transformers[transformer_id][bag_id]\n",
    "                X_transformed = transformer.transform(X)\n",
    "                voter = self.transformer_id_to_voters[transformer_id][bag_id]\n",
    "                vote = voter.predict_proba(X_transformed)\n",
    "                vote_per_bag_id.append(vote)\n",
    "            vote_per_transformer_id.append(np.mean(vote_per_bag_id, axis=0))\n",
    "            decider_vote = np.mean(vote_per_transformer_id, axis=0)\n",
    "            \n",
    "        return decider_vote #, vote_per_transformer_id, vote_per_bag_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ns in task_10_sample: # 100 to 5000 sample size for task 10\n",
    "    \n",
    "#     # size of estimation and validation sample sets\n",
    "#     estimation_sample_no = ceil(estimation_set*ns)\n",
    "#     validation_sample_no = ns - estimation_sample_no\n",
    "#     #\n",
    "#     print(estimation_sample_no)\n",
    "#     print(validation_sample_no)\n",
    "#     #\n",
    "\n",
    "#     # repeat `rep` times\n",
    "#     for rep in range(reps):\n",
    "#         print(\"doing {} samples for {} th rep\".format(ns,rep))\n",
    "        \n",
    "#         ## estimation\n",
    "#         l2f = LifelongClassificationForest(n_estimators=ntrees)\n",
    "        \n",
    "#         # training l2f on first 9 tasks\n",
    "#         for task in range(9):\n",
    "#             indx = np.random.choice(num_points_per_task, num_points_per_forest, replace=False)\n",
    "#             l2f.add_task(\n",
    "#                 train_x_across_task[task][indx], \n",
    "#                 train_y_across_task[task][indx]) \n",
    "#                 #max_depth=ceil(log2(num_points_per_forest)))\n",
    "        \n",
    "#         # 10th task...\n",
    "        \n",
    "#         task_10_train_indx = np.random.choice(num_points_per_task, ns, replace=False)\n",
    "\n",
    "#         l2f.add_task(\n",
    "#             train_x_across_task[9][task_10_train_indx[:estimation_sample_no]], \n",
    "#             train_y_across_task[9][task_10_train_indx[:estimation_sample_no]]\n",
    "#             #max_depth=ceil(log2(estimation_sample_no)),\n",
    "#             )\n",
    "\n",
    "        \n",
    "#         ## L2F validation\n",
    "#         for task_num in range(9):\n",
    "#             posterior_per_tree = l2f.predict_proba(\n",
    "#                 train_x_across_task[9][task_10_train_indx[estimation_sample_no:]],\n",
    "#                 task_id=task_num\n",
    "#                 )\n",
    "#             print(posterior_per_tree.shape)\n",
    "        \n",
    "#         #posteriors_across_trees = estimate_posteriors(\n",
    "#         #    l2f,\n",
    "#         #    train_x_across_task[9][task_10_train_indx[estimation_sample_no:]],\n",
    "#         #    representation=[0,1,2,3,4,5,6,7,8],\n",
    "#         #    decider=9\n",
    "#         #    )\n",
    "        \n",
    "#         posteriors_across_trees = posteriors_across_trees.reshape(\n",
    "#             9*ntrees,\n",
    "#             validation_sample_no,\n",
    "#             10\n",
    "#             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "37\n",
      "doing 100 samples for 0 th rep\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-97b423598d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mtrain_x_across_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_10_train_indx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimation_sample_no\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mtask_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mtransformer_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#[0,1,2,3,4,5,6,7,8]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 )\n\u001b[1;32m     66\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposterior_per_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/proglearn-0.0.1-py3.7.egg/proglearn/progressive_learner.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, task_id, transformer_ids)\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0mdecider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_id_to_decider\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         return self.task_id_to_decider[task_id].predict_proba(\n\u001b[0;32m--> 735\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformer_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/proglearn-0.0.1-py3.7.egg/proglearn/deciders.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, transformer_ids)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mX_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mvoter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_id_to_voters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbag_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mvote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0mvote_per_bag_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mvote_per_transformer_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvote_per_bag_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/proglearn-0.0.1-py3.7.egg/proglearn/voters.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_label_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mnew_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvotes_per_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mvotes_per_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvotes_per_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvotes_per_example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minsert\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(arr, obj, values, axis)\u001b[0m\n\u001b[1;32m   4575\u001b[0m             raise IndexError(\n\u001b[1;32m   4576\u001b[0m                 \u001b[0;34m\"index %i is out of bounds for axis %i with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4577\u001b[0;31m                 \"size %i\" % (obj, axis, N))\n\u001b[0m\u001b[1;32m   4578\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4579\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "for ns in task_10_sample: # 100 to 5000 sample size for task 10\n",
    "    \n",
    "    # size of estimation and validation sample sets\n",
    "    estimation_sample_no = ceil(estimation_set*ns)\n",
    "    validation_sample_no = ns - estimation_sample_no\n",
    "    #\n",
    "    print(estimation_sample_no)\n",
    "    print(validation_sample_no)\n",
    "    #\n",
    "\n",
    "    # repeat `rep` times\n",
    "    for rep in range(reps):\n",
    "        print(\"doing {} samples for {} th rep\".format(ns,rep))\n",
    "        \n",
    "        ## estimation\n",
    "        \n",
    "        # use lower-level ProgressiveLearner instance\n",
    "        l2f = ClassificationProgressiveLearner(\n",
    "            default_transformer_class=TreeClassificationTransformer,\n",
    "            default_transformer_kwargs={},\n",
    "            default_voter_class=TreeClassificationVoter,\n",
    "            default_voter_kwargs={\n",
    "                \"finite_sample_correction\": False\n",
    "            },\n",
    "            default_decider_class=SimpleArgmaxAverage,\n",
    "            #default_decider_class=NOTAVERAGE,\n",
    "            default_decider_kwargs={},\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # training l2f on first 9 tasks\n",
    "        for task in range(9):\n",
    "            indx = np.random.choice(num_points_per_task, num_points_per_forest, replace=False)\n",
    "            cur_X = train_x_across_task[task][indx]\n",
    "            cur_y = train_y_across_task[task][indx]\n",
    "            l2f.add_task(\n",
    "                cur_X, \n",
    "                cur_y,\n",
    "                num_transformers = ntrees,\n",
    "                #max_depth=ceil(log2(num_points_per_forest)))\n",
    "                voter_kwargs={\"classes\": np.unique(cur_y),\"finite_sample_correction\": False},\n",
    "                decider_kwargs={\"classes\": np.unique(cur_y)}\n",
    "            )\n",
    "        \n",
    "        # 10th task...\n",
    "        \n",
    "        task_10_train_indx = np.random.choice(num_points_per_task, ns, replace=False)\n",
    "        cur_X = train_x_across_task[9][task_10_train_indx[:estimation_sample_no]]\n",
    "        cur_y = train_y_across_task[9][task_10_train_indx[:estimation_sample_no]]\n",
    "        l2f.add_task(\n",
    "            cur_X, \n",
    "            cur_y,\n",
    "            num_transformers = ntrees,\n",
    "            #max_depth=ceil(log2(estimation_sample_no)),\n",
    "            voter_kwargs={\"classes\": np.unique(cur_y),\"finite_sample_correction\": False},\n",
    "            decider_kwargs={\"classes\": np.unique(cur_y)}\n",
    "        )\n",
    "        \n",
    "        ## L2F validation\n",
    "        for tasks in range(9):\n",
    "            posterior_per_tree = l2f.predict_proba(\n",
    "                train_x_across_task[9][task_10_train_indx[estimation_sample_no:]],\n",
    "                task_id=tasks,\n",
    "                transformer_ids=[0] #[0,1,2,3,4,5,6,7,8]\n",
    "                )\n",
    "            print(posterior_per_tree)\n",
    "            print(posterior_per_tree.shape)\n",
    "        \n",
    "        #posteriors_across_trees = estimate_posteriors(\n",
    "        #    l2f,\n",
    "        #    train_x_across_task[9][task_10_train_indx[estimation_sample_no:]],\n",
    "        #    representation=[0,1,2,3,4,5,6,7,8],\n",
    "        #    decider=9\n",
    "        #    )\n",
    "        \n",
    "        posteriors_across_trees = posteriors_across_trees.reshape(\n",
    "            9*ntrees,\n",
    "            validation_sample_no,\n",
    "            10\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        error_across_trees = np.zeros(9*ntrees)\n",
    "        validation_target = train_y_across_task[9][task_10_train_indx[estimation_sample_no:]]\n",
    "        for tree in range(9*ntrees):\n",
    "            res = np.argmax(posteriors_across_trees[tree],axis=1) + 90\n",
    "            error_across_trees[tree] = 1-np.mean(\n",
    "                validation_target==res\n",
    "            )\n",
    "\n",
    "        best_50_tree = np.argsort(error_across_trees)[:50]\n",
    "        best_25_tree = best_50_tree[:25]\n",
    "        \n",
    "        ## uf trees validation\n",
    "        posteriors_across_trees = estimate_posteriors(\n",
    "            l2f,\n",
    "            train_x_across_task[9][task_10_train_indx[estimation_sample_no:]],\n",
    "            representation=9,\n",
    "            decider=9\n",
    "            )[0]\n",
    "\n",
    "        error_across_trees = np.zeros(ntrees)\n",
    "        validation_target = train_y_across_task[9][task_10_train_indx[estimation_sample_no:]]\n",
    "        for tree in range(ntrees):\n",
    "            res = np.argmax(posteriors_across_trees[tree],axis=1) + 90\n",
    "            error_across_trees[tree] = 1-np.mean(\n",
    "                validation_target==res\n",
    "            )\n",
    "        best_25_uf_tree = np.argsort(error_across_trees)[:25]\n",
    "\n",
    "        ## evaluation\n",
    "        posteriors_across_trees = estimate_posteriors(\n",
    "            l2f,\n",
    "            test_x_across_task[9],\n",
    "            representation=[0,1,2,3,4,5,6,7,8],\n",
    "            decider=9\n",
    "            )\n",
    "        posteriors_across_trees = posteriors_across_trees.reshape(\n",
    "            9*ntrees,\n",
    "            1000,\n",
    "            10\n",
    "            )\n",
    "        # RECRUITING\n",
    "        recruiting_posterior = np.mean(posteriors_across_trees[best_50_tree],axis=0)\n",
    "        res = np.argmax(recruiting_posterior,axis=1) + 90\n",
    "        recruiting[rep] = 1 - np.mean(\n",
    "                test_y_across_task[9]==res\n",
    "            )\n",
    "        # BUILDING\n",
    "        building_res = l2f.predict(\n",
    "            test_x_across_task[9],\n",
    "            representation=[0,1,2,3,4,5,6,7,8,9],\n",
    "            decider=9\n",
    "        )\n",
    "        building[rep] = 1 - np.mean(\n",
    "                test_y_across_task[9]==building_res\n",
    "            )\n",
    "        # UF\n",
    "        uf_res = l2f.predict(\n",
    "            test_x_across_task[9],\n",
    "            representation=9,\n",
    "            decider=9\n",
    "        )\n",
    "        uf[rep] = 1 - np.mean(\n",
    "                test_y_across_task[9]==uf_res\n",
    "            )\n",
    "        # HYBRID\n",
    "        posteriors_across_trees_hybrid_uf = estimate_posteriors(\n",
    "            l2f,\n",
    "            test_x_across_task[9],\n",
    "            representation=9,\n",
    "            decider=9\n",
    "            )[0]\n",
    "        \n",
    "        hybrid_posterior_all = np.concatenate(\n",
    "            (\n",
    "                posteriors_across_trees[best_25_tree],\n",
    "                posteriors_across_trees_hybrid_uf[best_25_uf_tree]\n",
    "            ),\n",
    "            axis=0\n",
    "        )\n",
    "        hybrid_posterior = np.mean(\n",
    "            hybrid_posterior_all,\n",
    "            axis=0\n",
    "        )\n",
    "        hybrid_res = np.argmax(hybrid_posterior,axis=1) + 90\n",
    "        hybrid[rep] = 1 - np.mean(\n",
    "                test_y_across_task[9]==hybrid_res\n",
    "            )\n",
    "    mean_accuracy_dict['hybrid'].append(np.mean(hybrid))\n",
    "    std_accuracy_dict['hybrid'].append(np.std(hybrid,ddof=1))\n",
    "\n",
    "    mean_accuracy_dict['building'].append(np.mean(building))\n",
    "    std_accuracy_dict['building'].append(np.std(building,ddof=1))\n",
    "\n",
    "    mean_accuracy_dict['recruiting'].append(np.mean(recruiting))\n",
    "    std_accuracy_dict['recruiting'].append(np.std(recruiting,ddof=1))\n",
    "\n",
    "    mean_accuracy_dict['UF'].append(np.mean(uf))\n",
    "    std_accuracy_dict['UF'].append(np.std(uf,ddof=1))\n",
    "\n",
    "summary = (mean_accuracy_dict,std_accuracy_dict)\n",
    "\n",
    "with open('result/recruitment_exp_'+str(num_points_per_forest)+'.pickle','wb') as f:\n",
    "    pickle.dump(summary,f)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "mean_error = unpickle('recruitment_result/recruitment_mean.pickle')\n",
    "std_error = unpickle('recruitment_result/recruitment_std.pickle')\n",
    "ns = 10*np.array([50, 100, 200, 350, 500])\n",
    "colors = sns.color_palette('Set1', n_colors=mean_error.shape[0]+2)\n",
    "\n",
    "#labels = ['recruiting', 'Uncertainty Forest', 'hybrid', '50 Random', 'BF', 'building']\n",
    "labels = ['hybrid', 'building', 'recruiting','50 Random', 'BF', 'Uncertainty Forest' ]\n",
    "not_included = ['BF', '50 Random']\n",
    "    \n",
    "adjust = 0\n",
    "for i, error_ in enumerate(mean_error[:-1]):\n",
    "    if labels[i] in not_included:\n",
    "        adjust +=1\n",
    "        continue\n",
    "    ax.plot(ns, mean_error[i], c=colors[i+1-adjust], label=labels[i])\n",
    "    ax.fill_between(ns, \n",
    "            mean_error[i] + 1.96*std_error[i], \n",
    "            mean_error[i] - 1.96*std_error[i], \n",
    "            where=mean_error[i] + 1.96*std_error[i] >= mean_error[i] - 1.96*std_error[i], \n",
    "            facecolor=colors[i+1-adjust], \n",
    "            alpha=0.15,\n",
    "            interpolate=False)\n",
    "\n",
    "ax.plot(ns, mean_error[-1], c=colors[0], label=labels[-1])\n",
    "ax.fill_between(ns, \n",
    "        mean_error[-1] + 1.96*std_error[-1], \n",
    "        mean_error[-1] - 1.96*std_error[-1], \n",
    "        where=mean_error[-1] + 1.96*std_error[i] >= mean_error[-1] - 1.96*std_error[-1], \n",
    "        facecolor=colors[0], \n",
    "        alpha=0.15,\n",
    "        interpolate=False)\n",
    "\n",
    "\n",
    "#ax.set_title('CIFAR Recruitment Experiment', fontsize=30)\n",
    "ax.set_ylabel('Accuracy', fontsize=28)\n",
    "ax.set_xlabel('Number of Task 10 Samples', fontsize=30)\n",
    "ax.tick_params(labelsize=28)\n",
    "ax.set_ylim(0.325, 0.575)\n",
    "ax.set_title(\"CIFAR Recruitment\",fontsize=30)\n",
    "ax.set_xticks([500, 2000, 5000])\n",
    "ax.set_yticks([0.35, 0.45, 0.55])\n",
    "\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "right_side = ax.spines[\"right\"]\n",
    "right_side.set_visible(False)\n",
    "top_side = ax.spines[\"top\"]\n",
    "top_side.set_visible(False)\n",
    "\n",
    "plt.savefig('figs/recruit.pdf', dpi=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
