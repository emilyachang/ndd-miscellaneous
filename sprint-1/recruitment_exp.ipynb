{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recruitment Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developing lifelong learning algorithms, prior work has involved two main approaches: building and reallocating. Building involves adding new resources to support the arrival of new data, whereas reallocation involves compression of representations to make room for new ones. However, biologically, there is a spectrum between these two modes.\n",
    "\n",
    "In order to examine whether current resources could be better leveraged, we test a range of approaches: **recruitment** of the best-performing existing trees, **building** new trees completely (the default approach that our L2F uses), ignoring all prior trees (essentially an uncertainty forest), and a **hybrid** between building and recruitment.\n",
    "\n",
    "We examine the performance of these four approaches based on the available training sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from math import log2, ceil \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(action='once')\n",
    "\n",
    "\n",
    "from proglearn.progressive_learner import ProgressiveLearner\n",
    "from proglearn.forest import LifelongClassificationForest, UncertaintyForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR 10x10 Tasks\n",
    "\n",
    "The classification problem that we examine in this tutorial makes use of the CIFAR 10x10 dataset. This dataset contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 49s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# import data \n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "data_x = np.concatenate([X_train, X_test])\n",
    "data_x = data_x.reshape((data_x.shape[0], data_x.shape[1] * data_x.shape[2] * data_x.shape[3]))\n",
    "data_y = np.concatenate([y_train, y_test])\n",
    "data_y = data_y[:, 0]\n",
    "\n",
    "train_x_across_task, train_y_across_task, test_x_across_task, test_y_across_task = sort_data(\n",
    "    data_x,data_y,num_points_per_task\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(data_x, data_y, num_points_per_task, total_task=10, shift=1):\n",
    "    x = data_x.copy()\n",
    "    y = data_y.copy()\n",
    "    idx = [np.where(data_y == u)[0] for u in np.unique(data_y)]\n",
    "    train_x_across_task = []\n",
    "    train_y_across_task = []\n",
    "    test_x_across_task = []\n",
    "    test_y_across_task = []\n",
    "\n",
    "    batch_per_task=5000//num_points_per_task\n",
    "    sample_per_class = num_points_per_task//total_task\n",
    "    test_data_slot=100//batch_per_task\n",
    "\n",
    "    for task in range(total_task):\n",
    "        for batch in range(batch_per_task):\n",
    "            for class_no in range(task*10,(task+1)*10,1):\n",
    "                indx = np.roll(idx[class_no],(shift-1)*100)\n",
    "                \n",
    "                if batch==0 and class_no==task*10:\n",
    "                    train_x = x[indx[batch*sample_per_class:(batch+1)*sample_per_class],:]\n",
    "                    train_y = y[indx[batch*sample_per_class:(batch+1)*sample_per_class]]\n",
    "                    test_x = x[indx[batch*test_data_slot+500:(batch+1)*test_data_slot+500],:]\n",
    "                    test_y = y[indx[batch*test_data_slot+500:(batch+1)*test_data_slot+500]]\n",
    "                else:\n",
    "                    train_x = np.concatenate((train_x, x[indx[batch*sample_per_class:(batch+1)*sample_per_class],:]), axis=0)\n",
    "                    train_y = np.concatenate((train_y, y[indx[batch*sample_per_class:(batch+1)*sample_per_class]]), axis=0)\n",
    "                    test_x = np.concatenate((test_x, x[indx[batch*test_data_slot+500:(batch+1)*test_data_slot+500],:]), axis=0)\n",
    "                    test_y = np.concatenate((test_y, y[indx[batch*test_data_slot+500:(batch+1)*test_data_slot+500]]), axis=0)\n",
    "        \n",
    "        train_x_across_task.append(train_x)\n",
    "        train_y_across_task.append(train_y)\n",
    "        test_x_across_task.append(test_x)\n",
    "        test_y_across_task.append(test_y)\n",
    "\n",
    "    return train_x_across_task, train_y_across_task, test_x_across_task, test_y_across_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voter_predict_proba(voter, nodes_across_trees):\n",
    "            def worker(tree_idx):\n",
    "                #get the node_ids_to_posterior_map for this tree\n",
    "                node_ids_to_posterior_map = voter.tree_idx_to_node_ids_to_posterior_map[tree_idx]\n",
    "\n",
    "                #get the nodes of X\n",
    "                nodes = nodes_across_trees[tree_idx]\n",
    "\n",
    "                posteriors = []\n",
    "                node_ids = node_ids_to_posterior_map.keys()\n",
    "\n",
    "                #loop over nodes of X\n",
    "                for node in nodes:\n",
    "                    #if we've seen this node before, simply get the posterior\n",
    "                    if node in node_ids:\n",
    "                        posteriors.append(node_ids_to_posterior_map[node])\n",
    "                    #if we haven't seen this node before, simply use the uniform posterior \n",
    "                    else:\n",
    "                        posteriors.append(np.ones((len(np.unique(voter.classes_)))) / len(voter.classes_))\n",
    "                return posteriors\n",
    "\n",
    "            if voter.parallel:\n",
    "                return Parallel(n_jobs=-1)(\n",
    "                                delayed(worker)(tree_idx) for tree_idx in range(voter.n_estimators)\n",
    "                        )\n",
    "\n",
    "            else:\n",
    "                return [worker(tree_idx) for tree_idx in range(voter.n_estimators)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_posteriors(l2f, X, representation = 0, decider = 0):\n",
    "        l2f.check_task_idx_(decider)\n",
    "        \n",
    "        if representation == \"all\":\n",
    "            representation = range(l2f.n_tasks)\n",
    "        elif isinstance(representation, int):\n",
    "            representation = np.array([representation])\n",
    "        \n",
    "        def worker(transformer_task_idx):\n",
    "            transformer = l2f.transformers_across_tasks[transformer_task_idx]\n",
    "            voter = l2f.voters_across_tasks_matrix[decider][transformer_task_idx]\n",
    "\n",
    "            return voter_predict_proba(voter,transformer(X))\n",
    "        \n",
    "        '''if l2f.parallel:\n",
    "            posteriors_across_tasks = np.array(\n",
    "                        Parallel(n_jobs=-1)(\n",
    "                                delayed(worker)(transformer_task_idx) for transformer_task_idx in representation\n",
    "                        )\n",
    "                )    \n",
    "        else:'''\n",
    "        posteriors_across_tasks = np.array([worker(transformer_task_idx) for transformer_task_idx in representation])    \n",
    "\n",
    "        return posteriors_across_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "### Main hyperparameters ###\n",
    "############################\n",
    "ntrees = 50\n",
    "hybrid_comp_trees = 25\n",
    "estimation_set = 0.63\n",
    "validation_set= 1-estimation_set\n",
    "\n",
    "#num_points_per_task = 5000\n",
    "#num_points_per_forest = 500\n",
    "#reps = 30\n",
    "num_points_per_task = 100\n",
    "num_points_per_forest = 10\n",
    "reps = 5\n",
    "\n",
    "task_10_sample = 10*np.array([10, 50, 100, 200, 350, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrices for storing values\n",
    "hybrid = np.zeros(reps,dtype=float)\n",
    "building = np.zeros(reps,dtype=float)\n",
    "recruiting= np.zeros(reps,dtype=float)\n",
    "uf = np.zeros(reps,dtype=float)\n",
    "mean_accuracy_dict = {'hybrid':[],'building':[],'recruiting':[],'UF':[]}\n",
    "std_accuracy_dict = {'hybrid':[],'building':[],'recruiting':[],'UF':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from paper:\n",
    "\n",
    "train L2F on first nine CIFAR 10x10 tasks (50 trees/task, 500 samples/task)\n",
    "\n",
    "for 10th task:\n",
    "1. recruiting = select 50/450 existing trees that perform best on task 10\n",
    "2. building = train 50 new trees (L2F default)\n",
    "3. hybrid = build and recruit 25 trees\n",
    "4. UF = ignore prior trees\n",
    "\n",
    "should see L2F outperform others except @ 5k training samples: \"relative performance depends on available resources and sample size\"\n",
    "\n",
    "future work: \"investigate optimal strtegies or determining how to optimally leverage existing resources given a new task\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#from proglearn.progressive_learner import ClassificationProgressiveLearner\n",
    "from proglearn.transformers import TreeClassificationTransformer\n",
    "from proglearn.voters import TreeClassificationVoter\n",
    "from proglearn.deciders import SimpleArgmaxAverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions.recruitment_functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ns in task_10_sample: # 100 to 5000 sample size for task 10\n",
    "    \n",
    "#     # size of estimation and validation sample sets\n",
    "#     estimation_sample_no = ceil(estimation_set*ns)\n",
    "#     validation_sample_no = ns - estimation_sample_no\n",
    "#     #\n",
    "#     print(estimation_sample_no)\n",
    "#     print(validation_sample_no)\n",
    "#     #\n",
    "\n",
    "#     # repeat `rep` times\n",
    "#     for rep in range(reps):\n",
    "#         print(\"doing {} samples for {} th rep\".format(ns,rep))\n",
    "        \n",
    "#         ## estimation\n",
    "#         l2f = LifelongClassificationForest(n_estimators=ntrees)\n",
    "        \n",
    "#         # training l2f on first 9 tasks\n",
    "#         for task in range(9):\n",
    "#             indx = np.random.choice(num_points_per_task, num_points_per_forest, replace=False)\n",
    "#             l2f.add_task(\n",
    "#                 train_x_across_task[task][indx], \n",
    "#                 train_y_across_task[task][indx]) \n",
    "#                 #max_depth=ceil(log2(num_points_per_forest)))\n",
    "        \n",
    "#         # 10th task...\n",
    "        \n",
    "#         task_10_train_indx = np.random.choice(num_points_per_task, ns, replace=False)\n",
    "\n",
    "#         l2f.add_task(\n",
    "#             train_x_across_task[9][task_10_train_indx[:estimation_sample_no]], \n",
    "#             train_y_across_task[9][task_10_train_indx[:estimation_sample_no]]\n",
    "#             #max_depth=ceil(log2(estimation_sample_no)),\n",
    "#             )\n",
    "\n",
    "        \n",
    "#         ## L2F validation\n",
    "#         for task_num in range(9):\n",
    "#             posterior_per_tree = l2f.predict_proba(\n",
    "#                 train_x_across_task[9][task_10_train_indx[estimation_sample_no:]],\n",
    "#                 task_id=task_num\n",
    "#                 )\n",
    "#             print(posterior_per_tree.shape)\n",
    "        \n",
    "#         #posteriors_across_trees = estimate_posteriors(\n",
    "#         #    l2f,\n",
    "#         #    train_x_across_task[9][task_10_train_indx[estimation_sample_no:]],\n",
    "#         #    representation=[0,1,2,3,4,5,6,7,8],\n",
    "#         #    decider=9\n",
    "#         #    )\n",
    "        \n",
    "#         posteriors_across_trees = posteriors_across_trees.reshape(\n",
    "#             9*ntrees,\n",
    "#             validation_sample_no,\n",
    "#             10\n",
    "#             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "37\n",
      "doing 100 samples for 0 th rep\n",
      "False\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TreeClassificationVoter' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-3a84037c4f75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mtrain_x_across_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_10_train_indx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimation_sample_no\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mtask_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mtransformer_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 )\n\u001b[1;32m     66\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposterior_per_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/proglearn-0.0.1-py3.7.egg/proglearn/progressive_learner.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, task_id, transformer_ids)\u001b[0m\n",
      "\u001b[0;32m~/Documents/progressive-learning/tutorials/functions/recruitment_functions.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, transformer_ids)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinite_sample_correction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mvote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TreeClassificationVoter' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "for ns in task_10_sample: # 100 to 5000 sample size for task 10\n",
    "    \n",
    "    # size of estimation and validation sample sets\n",
    "    estimation_sample_no = ceil(estimation_set*ns)\n",
    "    validation_sample_no = ns - estimation_sample_no\n",
    "    #\n",
    "    print(estimation_sample_no)\n",
    "    print(validation_sample_no)\n",
    "    #\n",
    "\n",
    "    # repeat `rep` times\n",
    "    for rep in range(reps):\n",
    "        print(\"doing {} samples for {} th rep\".format(ns,rep))\n",
    "        \n",
    "        ## estimation\n",
    "        \n",
    "        # use lower-level ProgressiveLearner instance\n",
    "        l2f = ProgressiveLearner(\n",
    "            default_transformer_class=TreeClassificationTransformer,\n",
    "            default_transformer_kwargs={},\n",
    "            default_voter_class=TreeClassificationVoter,\n",
    "            default_voter_kwargs={\n",
    "                \"finite_sample_correction\": False\n",
    "            },\n",
    "            #default_decider_class=SimpleArgmaxAverage,\n",
    "            default_decider_class=fn.NOTAVERAGE,\n",
    "            default_decider_kwargs={},\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # training l2f on first 9 tasks\n",
    "        for task in range(9):\n",
    "            indx = np.random.choice(num_points_per_task, num_points_per_forest, replace=False)\n",
    "            cur_X = train_x_across_task[task][indx]\n",
    "            cur_y = train_y_across_task[task][indx]\n",
    "            l2f.add_task(\n",
    "                cur_X, \n",
    "                cur_y,\n",
    "                num_transformers = ntrees,\n",
    "                #max_depth=ceil(log2(num_points_per_forest)))\n",
    "                #voter_kwargs={\"classes\": np.unique(cur_y),\"finite_sample_correction\": False},\n",
    "                decider_kwargs={\"classes\": np.unique(cur_y)}\n",
    "            )\n",
    "        \n",
    "        # 10th task...\n",
    "        \n",
    "        task_10_train_indx = np.random.choice(num_points_per_task, ns, replace=False)\n",
    "        cur_X = train_x_across_task[9][task_10_train_indx[:estimation_sample_no]]\n",
    "        cur_y = train_y_across_task[9][task_10_train_indx[:estimation_sample_no]]\n",
    "        l2f.add_task(\n",
    "            cur_X, \n",
    "            cur_y,\n",
    "            num_transformers = ntrees,\n",
    "            #max_depth=ceil(log2(estimation_sample_no)),\n",
    "            #voter_kwargs={\"classes\": np.unique(cur_y),\"finite_sample_correction\": False},\n",
    "            decider_kwargs={\"classes\": np.unique(cur_y)}\n",
    "        )\n",
    "        \n",
    "        ## L2F validation\n",
    "        for tasks in range(9):\n",
    "            posterior_per_tree = l2f.predict_proba(\n",
    "                train_x_across_task[9][task_10_train_indx[estimation_sample_no:]],\n",
    "                task_id=tasks,\n",
    "                transformer_ids=[0,1,2,3,4,5,6,7,8]\n",
    "                )\n",
    "            print(posterior_per_tree.shape)\n",
    "        \n",
    "        #posteriors_across_trees = estimate_posteriors(\n",
    "        #    l2f,\n",
    "        #    train_x_across_task[9][task_10_train_indx[estimation_sample_no:]],\n",
    "        #    representation=[0,1,2,3,4,5,6,7,8],\n",
    "        #    decider=9\n",
    "        #    )\n",
    "        \n",
    "        posteriors_across_trees = posteriors_across_trees.reshape(\n",
    "            9*ntrees,\n",
    "            validation_sample_no,\n",
    "            10\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        error_across_trees = np.zeros(9*ntrees)\n",
    "        validation_target = train_y_across_task[9][task_10_train_indx[estimation_sample_no:]]\n",
    "        for tree in range(9*ntrees):\n",
    "            res = np.argmax(posteriors_across_trees[tree],axis=1) + 90\n",
    "            error_across_trees[tree] = 1-np.mean(\n",
    "                validation_target==res\n",
    "            )\n",
    "\n",
    "        best_50_tree = np.argsort(error_across_trees)[:50]\n",
    "        best_25_tree = best_50_tree[:25]\n",
    "        \n",
    "        ## uf trees validation\n",
    "        posteriors_across_trees = estimate_posteriors(\n",
    "            l2f,\n",
    "            train_x_across_task[9][task_10_train_indx[estimation_sample_no:]],\n",
    "            representation=9,\n",
    "            decider=9\n",
    "            )[0]\n",
    "\n",
    "        error_across_trees = np.zeros(ntrees)\n",
    "        validation_target = train_y_across_task[9][task_10_train_indx[estimation_sample_no:]]\n",
    "        for tree in range(ntrees):\n",
    "            res = np.argmax(posteriors_across_trees[tree],axis=1) + 90\n",
    "            error_across_trees[tree] = 1-np.mean(\n",
    "                validation_target==res\n",
    "            )\n",
    "        best_25_uf_tree = np.argsort(error_across_trees)[:25]\n",
    "\n",
    "        ## evaluation\n",
    "        posteriors_across_trees = estimate_posteriors(\n",
    "            l2f,\n",
    "            test_x_across_task[9],\n",
    "            representation=[0,1,2,3,4,5,6,7,8],\n",
    "            decider=9\n",
    "            )\n",
    "        posteriors_across_trees = posteriors_across_trees.reshape(\n",
    "            9*ntrees,\n",
    "            1000,\n",
    "            10\n",
    "            )\n",
    "        # RECRUITING\n",
    "        recruiting_posterior = np.mean(posteriors_across_trees[best_50_tree],axis=0)\n",
    "        res = np.argmax(recruiting_posterior,axis=1) + 90\n",
    "        recruiting[rep] = 1 - np.mean(\n",
    "                test_y_across_task[9]==res\n",
    "            )\n",
    "        # BUILDING\n",
    "        building_res = l2f.predict(\n",
    "            test_x_across_task[9],\n",
    "            representation=[0,1,2,3,4,5,6,7,8,9],\n",
    "            decider=9\n",
    "        )\n",
    "        building[rep] = 1 - np.mean(\n",
    "                test_y_across_task[9]==building_res\n",
    "            )\n",
    "        # UF\n",
    "        uf_res = l2f.predict(\n",
    "            test_x_across_task[9],\n",
    "            representation=9,\n",
    "            decider=9\n",
    "        )\n",
    "        uf[rep] = 1 - np.mean(\n",
    "                test_y_across_task[9]==uf_res\n",
    "            )\n",
    "        # HYBRID\n",
    "        posteriors_across_trees_hybrid_uf = estimate_posteriors(\n",
    "            l2f,\n",
    "            test_x_across_task[9],\n",
    "            representation=9,\n",
    "            decider=9\n",
    "            )[0]\n",
    "        \n",
    "        hybrid_posterior_all = np.concatenate(\n",
    "            (\n",
    "                posteriors_across_trees[best_25_tree],\n",
    "                posteriors_across_trees_hybrid_uf[best_25_uf_tree]\n",
    "            ),\n",
    "            axis=0\n",
    "        )\n",
    "        hybrid_posterior = np.mean(\n",
    "            hybrid_posterior_all,\n",
    "            axis=0\n",
    "        )\n",
    "        hybrid_res = np.argmax(hybrid_posterior,axis=1) + 90\n",
    "        hybrid[rep] = 1 - np.mean(\n",
    "                test_y_across_task[9]==hybrid_res\n",
    "            )\n",
    "    mean_accuracy_dict['hybrid'].append(np.mean(hybrid))\n",
    "    std_accuracy_dict['hybrid'].append(np.std(hybrid,ddof=1))\n",
    "\n",
    "    mean_accuracy_dict['building'].append(np.mean(building))\n",
    "    std_accuracy_dict['building'].append(np.std(building,ddof=1))\n",
    "\n",
    "    mean_accuracy_dict['recruiting'].append(np.mean(recruiting))\n",
    "    std_accuracy_dict['recruiting'].append(np.std(recruiting,ddof=1))\n",
    "\n",
    "    mean_accuracy_dict['UF'].append(np.mean(uf))\n",
    "    std_accuracy_dict['UF'].append(np.std(uf,ddof=1))\n",
    "\n",
    "summary = (mean_accuracy_dict,std_accuracy_dict)\n",
    "\n",
    "with open('result/recruitment_exp_'+str(num_points_per_forest)+'.pickle','wb') as f:\n",
    "    pickle.dump(summary,f)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "mean_error = unpickle('recruitment_result/recruitment_mean.pickle')\n",
    "std_error = unpickle('recruitment_result/recruitment_std.pickle')\n",
    "ns = 10*np.array([50, 100, 200, 350, 500])\n",
    "colors = sns.color_palette('Set1', n_colors=mean_error.shape[0]+2)\n",
    "\n",
    "#labels = ['recruiting', 'Uncertainty Forest', 'hybrid', '50 Random', 'BF', 'building']\n",
    "labels = ['hybrid', 'building', 'recruiting','50 Random', 'BF', 'Uncertainty Forest' ]\n",
    "not_included = ['BF', '50 Random']\n",
    "    \n",
    "adjust = 0\n",
    "for i, error_ in enumerate(mean_error[:-1]):\n",
    "    if labels[i] in not_included:\n",
    "        adjust +=1\n",
    "        continue\n",
    "    ax.plot(ns, mean_error[i], c=colors[i+1-adjust], label=labels[i])\n",
    "    ax.fill_between(ns, \n",
    "            mean_error[i] + 1.96*std_error[i], \n",
    "            mean_error[i] - 1.96*std_error[i], \n",
    "            where=mean_error[i] + 1.96*std_error[i] >= mean_error[i] - 1.96*std_error[i], \n",
    "            facecolor=colors[i+1-adjust], \n",
    "            alpha=0.15,\n",
    "            interpolate=False)\n",
    "\n",
    "ax.plot(ns, mean_error[-1], c=colors[0], label=labels[-1])\n",
    "ax.fill_between(ns, \n",
    "        mean_error[-1] + 1.96*std_error[-1], \n",
    "        mean_error[-1] - 1.96*std_error[-1], \n",
    "        where=mean_error[-1] + 1.96*std_error[i] >= mean_error[-1] - 1.96*std_error[-1], \n",
    "        facecolor=colors[0], \n",
    "        alpha=0.15,\n",
    "        interpolate=False)\n",
    "\n",
    "\n",
    "#ax.set_title('CIFAR Recruitment Experiment', fontsize=30)\n",
    "ax.set_ylabel('Accuracy', fontsize=28)\n",
    "ax.set_xlabel('Number of Task 10 Samples', fontsize=30)\n",
    "ax.tick_params(labelsize=28)\n",
    "ax.set_ylim(0.325, 0.575)\n",
    "ax.set_title(\"CIFAR Recruitment\",fontsize=30)\n",
    "ax.set_xticks([500, 2000, 5000])\n",
    "ax.set_yticks([0.35, 0.45, 0.55])\n",
    "\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "right_side = ax.spines[\"right\"]\n",
    "right_side.set_visible(False)\n",
    "top_side = ax.spines[\"top\"]\n",
    "top_side.set_visible(False)\n",
    "\n",
    "plt.savefig('figs/recruit.pdf', dpi=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
